{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82c66e32",
      "metadata": {
        "id": "82c66e32"
      },
      "source": [
        "# Ensuring Reliable Few-Shot Prompt Selection for LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e356214c",
      "metadata": {
        "id": "e356214c"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cleanlab/cleanlab-tools/blob/master/few_shot_prompt_selection/few_shot_prompt_selection.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c42988",
      "metadata": {
        "id": "91c42988"
      },
      "source": [
        "In this notebook, we prompt the Davinci LLM from OpenAI (the model underpinning GPT-3/ChatGPT) with few-shot prompts in an effort to classify the intent of customer service requests at a large bank. Following typical practice, we source the few-shot examples to include in the prompt template from an available dataset of human-labeled request examples. However, the resulting LLM predictions are unreliable — a close inspection reveals this is because real-world data is messy and error-prone.  LLM performance in this customer service intent classification task is only marginally boosted by manually modifying the prompt template to mitigate potentially noisy data. The LLM predictions become significantly more accurate if we instead use data-centric AI algorithms via Cleanlab Studio to ensure only high-quality few-shot examples are selected for inclusion in the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d802b04f",
      "metadata": {
        "id": "d802b04f"
      },
      "source": [
        "# Imports and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "66f4f859",
      "metadata": {
        "id": "66f4f859"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import openai, os\n",
        "from openai import OpenAI\n",
        "import string\n",
        "import random\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "def eval_preds(preds):\n",
        "    acc = accuracy_score(preds, test.label.values)\n",
        "    return \"Model Accuracy: \" + '{:.1%}'.format(acc)\n",
        "\n",
        "def tokens_per_prompt(prompt):\n",
        "    encoding = tiktoken.get_encoding(\"p50k_base\")\n",
        "    num_tokens = len(encoding.encode(prompt))\n",
        "    return num_tokens\n",
        "\n",
        "def cost_per_prompt(prompt):\n",
        "    cost_per_token = 0.02 / 1000\n",
        "    tokens = tokens_per_prompt(prompt)\n",
        "    cost = tokens * cost_per_token\n",
        "    return cost\n",
        "\n",
        "def cost_per_test_evaluation(examples_pool, test):\n",
        "    texts = test.text.values\n",
        "    cost = 0\n",
        "    examples = get_examples(examples_pool)\n",
        "    for text in texts:\n",
        "        prompt = get_prompt(examples_pool, text, examples)\n",
        "        prompt_cost = cost_per_prompt(prompt)\n",
        "        cost += prompt_cost\n",
        "    return \"${:,.2f}\".format(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd34610",
      "metadata": {
        "id": "0cd34610"
      },
      "source": [
        "# OpenAI API Key\n",
        "Replace with your own key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "0bc467b4",
      "metadata": {
        "id": "0bc467b4",
        "outputId": "3f361b05-82da-48be-fcd7-33a05b2261e0"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"sk-TZn2hZ39D7YDD8vKw8P28JhK0AYQbE7vI2WtyB80rwi3WcDi\",  # 或者 os.environ[\"OPENAI_API_KEY\"]\n",
        "    base_url=\"https://sg.uiuiapi.com/v1\",  # 注意最好加 /v1,\n",
        "    # model=\"gpt-3.5-turbo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "01bdefac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# neg = pd.read_csv(r'/root/reinforcement_commit/datasets/dataset.csv')\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv('./dataset.csv')\n",
        "# df.dropna(inplace=True)\n",
        "# label2id = {'Adaptive':0,'Perfective':1,'Corrective':2}\n",
        "# df = df.replace({\"labels\": label2id})\n",
        "# df.rename(columns={'labels':'label','diffs':'diff','msgs':'message'},inplace=True)\n",
        "# df\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edd6b39",
      "metadata": {
        "id": "0edd6b39"
      },
      "source": [
        "# Building the Few-Shot Prompt\n",
        "Few-shot prompting is a technique used in natural language processing that enables pretrained foundation models to perform complex tasks without any explicit training (i.e. updates to model parameters).  In few-shot prompting (also known as in-context learning), we provide a model with a limited number of input-output pairs, as part of a prompt template that is included in the prompt used to instruct the model how to handle a particular input.\n",
        "\n",
        "Our prompt will consist of a few pieces:\n",
        "- (optional) prefix\n",
        "- list of class labels to help LLM choose a valid class\n",
        "- (optional) 50 examples, one from each class\n",
        "- target text for LLM to classify"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16c8462",
      "metadata": {
        "id": "b16c8462"
      },
      "source": [
        "## Adding List of Classes for Valid Completions\n",
        "\n",
        "This text will go at the beginning of the prompt and will tell the LLM what the valid classes are so that it can consistently output a class. Without this, the LLM will not choose a valid class and output something not parsable.\n",
        "\n",
        "Here we can also add an optional `prefix` that we will use later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04af47e3",
      "metadata": {
        "id": "04af47e3"
      },
      "source": [
        "## Generate Entire Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "eaaa0956",
      "metadata": {},
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += (\n",
        "                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            )\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e487377d",
      "metadata": {
        "id": "e487377d",
        "outputId": "12f89d35-98ba-4f26-c860-a1c92f70e5b1"
      },
      "outputs": [],
      "source": [
        "# Helper to format the k-shot prompt with:\n",
        "# - prefix\n",
        "# - 1 example from each class\n",
        "# - target text for classification\n",
        "def generate_promot(row):\n",
        "    base_promot = \"\"\"\n",
        "    Please act as a commit classifier,\n",
        "    categorize git commit into three categories: Adaptive, Perfective, and Corrective.\n",
        "    The Adaptive category corresponds to commits that address the modifications to the project in order to adapt it to the new environment such as the feature addition.\n",
        "    The Perfective category corresponds to commits that address enhancement of the project, such as enhancement of performance and refactoring of source code.\n",
        "    The Corrective category corresponds to commits that address the fix of bugs and faults in the project.\n",
        "    I will provide you with the commit message and code diff for a commit,\n",
        "    and you need to give me the category label for this commit.\n",
        "    Please avoid any explanations and only provide the category label.\n",
        "    \"\"\"\n",
        "\n",
        "    commit_message_prompt = f\"commit message: {row['msgs']}\\n\"\n",
        "    commit_codediff_prompt = f\"code diff: {row['diffs']}\\n\"\n",
        "\n",
        "    prompt = base_promot + commit_message_prompt + commit_codediff_prompt\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    l, r = 0, len(commit_codediff_prompt) - 1\n",
        "    while l <= r:\n",
        "        mid = (l + r) // 2\n",
        "        prompt = base_promot + commit_message_prompt + commit_codediff_prompt[:mid]\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        ok = num_tokens_from_messages(messages) < 4096\n",
        "        if ok:\n",
        "            l = mid + 1\n",
        "        else:\n",
        "            r = mid - 1\n",
        "    prompt = base_promot + commit_message_prompt + commit_codediff_prompt[:r]\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4180a3c9",
      "metadata": {
        "id": "4180a3c9"
      },
      "source": [
        "## Query OpenAI LLM API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "9a40952b",
      "metadata": {
        "id": "9a40952b",
        "outputId": "c9319feb-bda6-49bf-ebc6-c9b3e92e7443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Perfective\n",
            "Perfective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Corrective\n",
            "Corrective\n",
            "Adaptive\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Adaptive\n",
            "Corrective\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Adaptive\n",
            "Perfective\n",
            "Perfective\n"
          ]
        }
      ],
      "source": [
        "# Helper method to prompt OpenAI LLM and get response.\n",
        "def get_response(user_content):\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 根据代理方文档修改\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# text = \"\\'How can I change my pin?\\'\"\n",
        "# examples = get_examples(examples_pool)\n",
        "# df = pd.read_csv('dataset.csv')\n",
        "\n",
        "result = []\n",
        "for i, row in val_df.iterrows():\n",
        "    user_content = generate_promot(row)\n",
        "    # print(user_content)\n",
        "    response =  get_response(user_content)\n",
        "    result.append(response)\n",
        "    # print(\"Model classified \", user_content, \" as \", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "35fe18fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1009      Adaptive\n",
              "721     Corrective\n",
              "538     Corrective\n",
              "69      Perfective\n",
              "1210    Perfective\n",
              "           ...    \n",
              "527       Adaptive\n",
              "650     Perfective\n",
              "1420    Corrective\n",
              "1386    Perfective\n",
              "637     Perfective\n",
              "Name: labels, Length: 267, dtype: object"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "549ec7a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Adaptive     0.5926    0.5393    0.5647        89\n",
            "  Corrective     0.7629    0.8222    0.7914        90\n",
            "  Perfective     0.4831    0.4886    0.4859        88\n",
            "\n",
            "    accuracy                         0.6180       267\n",
            "   macro avg     0.6129    0.6167    0.6140       267\n",
            "weighted avg     0.6139    0.6180    0.6152       267\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "print(classification_report(val_df['labels'],result,digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898021a9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
